---
title: 'Lecture 1: Basic Models in Matrix Algebra'
author: "ETB"
date: "3/29/2022"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# This script uses the %*% notation for matrix multiplication, which is part of the Matrix package
require(Matrix)
require(MASS)
```

## ANOVA in R: model specification

Here is a basic model with _y_ as the dependent measure and _X1_ as the independent variable (dummy coded).

```{r basic model setup}
# y is the name of the DV (by convention)
y = c(1:10)

# X is the design matrix. In this case, an ANOVA/t-test with 2 or 3 groups
X = matrix(c(rep(1,5), rep(0,10), rep(1,5)), ncol=2)  # Two groups
# X = matrix(c(rep(1,10), c(17, 15, 13, 11, 9, 7, 5, 3, 1,-3)), ncol=2)
#X1 = matrix(c(rep(1,3), rep(0,10), rep(1,3), rep(0,10), rep(1,4)), ncol=3, byrow=F) # Three groups

X
```

## ANOVA in R: model estimation

Once the model is specified as above, we have all the pieces of our $Y = Xb+e$ equation except for $b$. So we need to solve for that. We do so by:

1. Assuming $e$ is, on average, equal to 0. (This is a fundamental assumption of the GLM)
2. Solve for $b$ by dividing both sides by $X$.
3. (Division is the same as multiplying by the inverse.)

```{r basic model estimation}
# Inverse of X (actually the 'generalized' or pseudoinverse)
invx = ginv(X)

# Solve for beta using: beta = X^(-1) * y
beta = ginv(X) %*% y
beta

# The model part of the GLM equation (Y = Xb + e), Xb
model = X %*% beta
model

# The error part of the GLM equation (Y = Xb + e), e
e = y - model
e
```

## ANOVA in R: statistical tests

Now that we have all the pieces, we can use them to calculate a signal to noise ratio: variance explained by the model divided by variance left over. 

```{r basic model tests}

# Sum-of-squares error = e'e, or "t(e)" times e
ss_e = t(e) %*% e
ss_e

# Sum-of-squares model = model'model, after subtracting the mean of the DV
ss_m = t(model-mean(y)) %*% (model-mean(y))
ss_m

# Degrees-of-freedom error is the number of rows - number of columns (r-c) of the design matrix
df_e = dim(X)[1] - dim(X)[2]  # the "size" function of a matrix wants the matrix (X) and row (1) or column (2)
df_e

# Degrees-of-freedom model is the number of columns in X (equivalent to # of rows in the betas) minus 1
df_m = dim(X)[2] - 1
df_m

# Mean-squared error = sum-of-squares error divided by degrees-of-freedom error
ms_e = ss_e / df_e
ms_e

# Mean-squared model = sum-of-squares model divided by degrees-of-freedom model
ms_m = ss_m / df_m
ms_m

# F is a signal-to-noise ratio where signal = mean-squared model and noise = mean-squared error
F = ms_m / ms_e
F

# p-value is the probability of obtainining an F-value of "F" or greater from a F-distribution
# arguments are f, df_m, df_e, and whether to take the P(F>f) [lower.tail=FALSE] or P(F<f) [lower.tail=TRUE]
pval = pf(F, df_m, df_e, lower.tail = FALSE)
pval

# Now compare with your by-hand or R calculations

```

The easy way:
```{r easy way}

group = c(rep(1,5), rep(2,5))
easy_way = lm(y ~ factor(group))
# easy_way = lm(y~c(17, 15, 13, 11, 9, 7, 5, 3, 1,-3))

summary(easy_way)

# Is a t-test the same as an F-test? Compare and contrast
# HINT: square the t-value...
t.test(y~group)

```

## What if we wanted to do regression?

Regression is _exactly the same_, as long as you change your design matrix accordingly. Note the inclusion of a column of all "1"s representing the _intercept_. The second column represents the predictor. (This makes sense because regressions with one predictor have two parameters.)

```{r regression model}

# Regression
pred = c(17, 15, 13, 11, 9, 7, 5, 3, 1,-3)  # Likert-style continuous predictor (e.g., neuroticism)
X = matrix(c(rep(1,10), pred), byrow=0, ncol=2)  # Regression design matrix: column of 1s (for intercept), predictor
X

```

## Regression in R: estimation and stats

Now you can re-run the _exact same code_ as above to estimate the model and conduct the tests. Yup. The solution code (inverting X, multiplying both sides by the inverse of $X$, calculating $e$, the $df$, the stats, etc) are all the same.

```{r regression estimation + stats}
invx = ginv(X)

# Solve for beta using: beta = X^(-1) * y
beta = ginv(X) %*% y

# The model part of the GLM equation (Y = Xb + e), Xb
model = X %*% beta

# The error part of the GLM equation (Y = Xb + e), e
e = y - model

# Sum-of-squares error = e'e, or "t(e)" times e
ss_e = t(e) %*% e

# Sum-of-squares model = model'model, after subtracting the mean of the DV
ss_m = t(model-mean(y)) %*% (model-mean(y))

# Degrees-of-freedom error is the number of rows - number of columns (r-c) of the design matrix
df_e = dim(X)[1] - dim(X)[2]  # the "size" function of a matrix wants the matrix (X) and row (1) or column (2)

# Degrees-of-freedom model is the number of columns in X (equivalent to # of rows in the betas) minus 1
df_m = dim(X)[2] - 1

# Mean-squared error = sum-of-squares error divided by degrees-of-freedom error
ms_e = ss_e / df_e

# Mean-squared model = sum-of-squares model divided by degrees-of-freedom model
ms_m = ss_m / df_m

# F is a signal-to-noise ratio where signal = mean-squared model and noise = mean-squared error
F = ms_m / ms_e
F

# p-value is the probability of obtainining an F-value of "F" or greater from a F-distribution
# arguments are f, df_m, df_e, and whether to take the P(F>f) [lower.tail=FALSE] or P(F<f) [lower.tail=TRUE]
pval = pf(F, df_m, df_e, lower.tail = FALSE)
pval

```

## Plot the regression results

You can plot the regression results thusly:

```{r plot regression}
xrange = c(-1:13)
plot(pred, y, col="blue", ylim=c(0,12), ylab = "DV (y)", xlab="IV (pred)")
lines(xrange, beta[1] + beta[2]*xrange)
```
