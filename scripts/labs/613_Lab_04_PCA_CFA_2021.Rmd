---
title: "PSY613_Lab4_PCAandCFA_Spring2020"
output: word_document
---
```{r}
# load packages
library(rio) # for importing the data
library(psych) # for many many functions
library(OpenMx) # for matrix algebra optimization and SEM
library(tidyverse) # for using dplyr functions and ggplot
library(here)

# import data
dat <- import(here("data", "613_Lab04_PCA_2021.sav"), 
                   setclass = "tibble")
```

# Check your data
```{r}
# look at first 6 rows of data
head(dat)
```

###############################
# PRINCIPAL COMPONENTS ANALYSIS
###############################

# Part 1: Get correlation matrix
```{r}
# get a correlation matrix
orig_cor <- data.frame(cor(dat, use = "pairwise.complete.obs")) # use specifies how to handle missing values (spoiler: we don't have any)

round(orig_cor, 2)

# with larger correlation matrices, it can be nice to look at a heatmap

## convert the correlation matrix to long format
orig_cor_long <- orig_cor %>%
  rownames_to_column(var = "var1") %>%
  gather(key = "var2", value = "corr", -var1)

## look at the melted correlation matrix
head(orig_cor_long)

# plot heatmap
ggplot(data = orig_cor_long, aes(x = var1, y = var2, fill = corr)) + 
  geom_tile()
```

# What does it mean to transform data? (Note: This is just a demonstration. This is not PCA.)

```{r}
# let's pretend we just have 2 variables
dat2 <- as.data.frame(dat[, c(15, 17)])

# right now, our variables are Sadness and Upset. 
ggplot(dat2, aes(x = Sadness, y = Upset)) + 
  geom_point(alpha = .3) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  scale_x_continuous(breaks = seq(-5, 5, by = 0.5), limits = c(-5, 5)) +
  scale_y_continuous(breaks = seq(-5, 5, by = 0.5), limits = c(-5, 5))

# Transformation 1: Take each subject's 2 scores and transform it to be:
# Score 1 = 0.5  * Sadness
# Score 2 = 0.75 * Upset
scaleMat    <- cbind(c(1/2, 0), c(0, 3/4))
scaleddat2 <- as.matrix(dat2) %*% scaleMat
colnames(scaleddat2) <- c("Sadness","Upset")
ggplot(as.data.frame(scaleddat2), aes(x = Sadness, y = Upset)) + 
  geom_point(alpha = .3) + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  scale_x_continuous(breaks = seq(-5, 5, by = 0.5), limits = c(-5, 5)) +
  scale_y_continuous(breaks = seq(-5, 5, by = 0.5), limits = c(-5, 5))

# Transformation 2: Take each subject's 2 scores and transform it as follows:
# Score 1 =  0.5 * Sadness + 0.25 * Upset
# Score 2 = -0.5 * Sadness + 0.25 * Upset 
# In this case, our two variables won't really be Sadness and Upset anymore; each score will be a blend of both

transMat <- cbind(c(.5, .25), c(-.5, .25))
transdat2 <- as.matrix(dat2) %*% transMat
colnames(transdat2) <- c("Score1","Score2")
ggplot(as.data.frame(transdat2), aes(x = Score1, y = Score2)) + 
  geom_point(alpha = .3) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  scale_x_continuous(breaks = seq(-5, 5, by = 0.5), limits = c(-5, 5)) +
  scale_y_continuous(breaks = seq(-5, 5, by = 0.5), limits = c(-5, 5))

# It's the same data -- and the 2 dimensions are preserved -- but now it's represented with a different underlying coordinate system.

# When PCA transforms your data, this is essentially what it's doing: transforming your data to a NEW 17 dimensions/components, with the first component carrying most of the variance, followed by the second component, etc.

# How does PCA decide how to transform your data? It uses the "eigenvectors" of the original correlation matrix! This part might feel magical, but just trust the work of many smart mathematicians.

# Once the data is transformed into PCA components, YOUR job is to decide how many of the new components to keep. Maybe only a few of them actually capture the variability in the data, and you can reduce the dimensionality of your data.
```

# Part 2: Unrotated PCA
### Helpful for determining the # of factors to extract
```{r}
pca <- princomp(dat, cor = TRUE) #cor=TRUE makes the calculation based on correlation matrix rather than the covariance matrix

# here are your transformed data (313 subs x 17 new variables)
pca$scores # not very useful but helpful to understand what PCA is doing

# summary of the variance accounted for by each component, derived from eigenvalues 
summary(pca)
# but because eigenvalues = variances, not standard deviations, do this to get eigenvalues:
pca$sdev^2

# scree plot of eigenvalues:
plot(pca, type = "lines", main = "Scree Plot") 

# here's another way to get the original eigenvalues and eigenvectors
eigens <- eigen(orig_cor)
eigens$values # these are the original (unrotated) eigenvalues
pca$sdev^2 # compare this to the eigenvalues 
# eigen() output also gives you the eigenvectors:
eigens$vectors
```

# Part 3: Exploring component/factor loadings
```{r}
# Principal component loadings (these are the eigenvectors; they are not actually the component loadings)
loadings(pca)

# Here's a cool way to visualize your new data
biplot(pca)

# How many components do we want to keep? Let's go with 3 for now. Just look at the loadings for the 
# first 3 components. They're the only ones with eigenvalues > 1
pca$loadings[,1:3] 

# Computing weighted loadings: 
# Weighted loadings = loadings * sqrt(eigenvalue)
loadings <- pca$loadings[,1:3]

# The "sdev" variable from pca is exactly what you need: the sqrt of the variance/eigenvalue.
weights <- vec2diag(pca$sdev[1:3]) # create a diagonal matrix of sdev values. Multiplying by this matrix = multiplying each column of your loadings by sdev.

# component loadings 
loadings[, 1:3] %*% weights

loadings(principal(orig_cor, nfactors = 17, rotate = "none"))
```

# Part 4: Reproduce correlation matrix with your selected components (in our case, 3 components) and compare to original correlation matrix
```{r}
reproduced_cor <- eigens$vectors[ , 1:3] %*% diag(eigens$values[1:3]) %*% t(eigens$vectors[ , 1:3])
round(reproduced_cor, 2)
resid_cor <- round(orig_cor - reproduced_cor, 2) # these are the residuals (the difference between original correlations, and the correlations we get from just the three best eigen vectors and values)
resid_cor

#what do small residulas indicate?

sqrt(mean(as.matrix(resid_cor^2))) # RMSA 

#What would we need to change about this line of code for reproduced_cor==orig_cor??
reproduced_cor <- eigens$vectors[ , 1:3] %*% diag(eigens$values[1:3]) %*% t(eigens$vectors[ , 1:3])
```

# Part 5: PCA, with orthogonal rotation
```{r}
pca_var <- principal(dat, rotate = "varimax", nfactors = 3, missing = TRUE)
summary(pca_var)
pca_var$loadings
pca_var$residual

# what about with just 2 components?
pca_var2 <- principal(dat, rotate = "varimax", nfactors = 2, missing = TRUE)
summary(pca_var2)
pca_var2$loadings
pca_var2$residual
plot(pca_var2$loadings, main = "Loadings for 2 Component Solution", xlab = "Component 1", ylab = "Component 2")
```
# Part 6: PCA, with oblique rotation
```{r}
pca_pro <- principal(dat, rotate = "promax", nfactors = 3, missing = TRUE)
summary(pca_pro)
pca_pro$loadings
pca_pro$residual
```

###############################
# CONFIRMATORY FACTOR ANALYSIS
###############################
```{r}
# clear the environment
rm(list = ls())

# read in the data file
dat <- import(here("data", "613_Lab04_FA_2021.sav"), setclass = "tibble") 

# look at the data
head(dat)

# get the lavaan package, which is a nice SEM package.
#install.packages("lavaan")
library(lavaan)

# Describe our model to lavaan. See http://lavaan.ugent.be/tutorial/cfa.html for an example.

# Note that using "=~" tells it we want to make a latent variable. Whatever we write to the left of that symbol will be the name of the latent factor.
# You can read "=~" as "is measured by" (e.g. "VS is measured by VS1, VS2, VS3, and VS4").

model <- '
VS  =~ VS1 + VS2 + VS3 + VS4
STM =~ STM1 + STM2 + STM3 + STM4
SC  =~ SC1 + SC2 + SC3 + SC4
'

# note that the whole model (all of the equations) needs to be surrounded by single quotes. 

# lavaan has several built-in functions for common kinds of SEM problems, such as CFA. 
# The cfa() function has useful defaults, like automatically allowing the latent variables to covary.
fit <- cfa(model, data = dat)
# this creates a big complicated object, called "fit", which includes all the information about our output

# this gives us all the parameter estimates and the fit statistics (need to say fit.measures=TRUE).
summary(fit, fit.measures = TRUE)

 # Does the model fit well? See this webpage for help interpreting common measures of model fit: http://davidakenny.net/cm/fit.htm

# What are the parameter estimates?

# Check out your reproduced covariance matrix http://lavaan.ugent.be/tutorial/inspect.html
fitted(fit)

#observed cov matrix
round(cov(dat), 3)

# and the residual cov matrix (observed - reproduced)
resid(fit)

# to get a figure of your model, use semPaths in the semPlot package
#install.packages("semPlot")
library(semPlot)
semPaths(fit)
# check out ?semPaths - there are a ton of options that are fun to play around with
```
