---
title: "PSY 613: Lab 5, Midterm Review"
author: ""
date: "4/29/2022"
output: 
  html_document:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r, message = FALSE, warning = FALSE}
library(rio)
library(tidyverse)
library(MASS)
library(psych)
library(broom)
library(reghelper)
```

# Matrix Algebra by Hand

## Helpful Notation

vectors are created by just using `c()`
```{r}
vector <- c(1, 2, 3, 4, 5)
vector
```

matrices are created by using `matrix()`
```{r}
x_mat <- matrix(c(1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 
                  0, 0, 0, 0, 0, 1, 1, 1, 1, 1), 
                nrow = 10, ncol = 2, byrow = FALSE)
x_mat
```

matrix operations: Transpose `t()`
```{r}
t(x_mat)
```

matrix operations: Matrix Multiplication `%*%`
```{r}
x_mat %*% t(x_mat)
```

matrix operations: Inverse `ginv()` from the `MAASS` library
```{r}
# solve() for true inverse

MASS::ginv(x_mat)
```

matrix operations: Addition and Subtraction
```{r}
x_mat + x_mat
x_mat - x_mat
```

## Tips and Reminders

* You have to show your work by hand, of course, but you can (and should) check your answers with computer calculations as you go. For example, check that your betas are accurate before you move on to calculate the error vector. 

Representing an ANOVA in matrix algebra format:

* Y is a single column vector containing participants' scores on the DV
* X is the design matrix that represents which group participants belonged to
* B is a vector containing the betas, or parameter estimates, that you solve 
* e is a vector of error terms, one for each subject (so it will be the same dimensions as Y)

## Review Questions:

Scenario: Imagine you're testing the effect of type of feedback (IV) on students' math performance (DV). You randomly assign participants to receive either positive, negative, or neutral feedback. The four students who received positive feedback got 7, 7, 8, and 6 problems correct on their math test. The four students who received negative feedback scored 3, 4, 3, and 2 on the math test. The four students who received neutral feedback scored 5, 6, 5, and 4.

Write the GLM equation representing how type of feedback predicts math performance in matrix algebra form. 

Y = XB + e

*Q: What does the Y vector look like?*
\[Y = \begin{bmatrix}
7 \\
7 \\
8 \\
6 \\
3 \\
4 \\
3 \\
2 \\
5 \\
6 \\
5 \\
4 \\
\end{bmatrix}
\]

```{r}
Y <- matrix(data = c(7, 7, 8, 6, 3, 4, 3, 2, 5, 6, 5, 4),
                     nrow = 12, ncol = 1, byrow = TRUE)
Y
```


*Q: What does the design matrix, X, look like?*
\[X=\begin{bmatrix}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 1 \\
0 & 0 & 1 \\
0 & 0 & 1 \\
\end{bmatrix}
\]

```{r}
X <- matrix(data = c(1, 0, 0,
                    1, 0, 0,
                    1, 0, 0,
                    1, 0, 0,
                    0, 1, 0,
                    0, 1, 0,
                    0, 1, 0,
                    0, 1, 0,
                    0, 0, 1, 
                    0, 0, 1,
                    0, 0, 1,
                    0, 0, 1), 
           nrow = 12, ncol = 3, byrow = TRUE)
X
```


Recall that the inverse of a matrix, $X$, is the matrix that returns the identity matrix when multiplied by $X$. In other words, $X^{-1}X = I$

Normally, B = Y/X, but since we're walking through matrices and there is no division in matrix algebra, that's why we use the inverse of X and multiply it by Y.

As you think through how to do this by hand, remember that X is a 3x12 matrix. To multiply X^-1, we need the inner dimensions to match. So your first step is to transpose X into a 3x12 matrix.

x^1  *  x
3x12 * 12x3 ... 3x3

Our resulting identity matrix will be a 3x3 matrix.

But there's one more step to get the answer, because currently you'd get:
\[\begin{bmatrix}
4 & 0 & 0 \\
0 & 4 & 0 \\
0 & 0 & 4 \\
\end{bmatrix}
\]

And we want to make sure our identity matrix would have 4's, not 1's, along the diagonal.

*Q: So what is $X^{-1}$ for this scenario?*   
\[X^{-1}=\begin{bmatrix}
1/4 & 1/4 & 1/4 & 1/4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1/4 & 1/4 & 1/4 & 1/4 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1/4 & 1/4 & 1/4 & 1/4 \\
\end{bmatrix}
\]

```{r check inverse in r}
ginv(X)
#this confirms 1/4 is the correct value
```


*Q: Solve for the parameter estimates contained in vector B.* 
```{r}
B <- ginv(X) %*% Y
B
```


*Q: How do you calculate e, the vector of error terms? Calculate them.*
A: $e = Y - XB$
```{r}
e <- Y - (X %*% B)
e
```


## Solving for Regression Model with Matrix Algebra

Let's consider a scenario where you're predicting a continuous outcome variable from a continuous predictor variable. For example, let's predict mental flexibility (ability to easily switch between tasks) from hours spent playing video games.

Mental flexibility: 16, 9, 4, 3, 18
Hours spent playing video games: 8, 5, 2, 0, 10

Construct the matrices for Y and X.
```{r}
Y <- matrix(data = c(16, 9, 4, 3, 18),
            nrow = 5, ncol = 1, byrow = TRUE)
Y
X <- matrix(data = c(1, 1, 1, 1, 1, 
                     8, 5, 2, 0, 10),
            nrow = 5, ncol = 2, byrow = FALSE)
X
```

*Q: What does the design matrix, X, look like when you have a continuous predictor variable?*
A column of ones for estimating b0 (y-intercept) and a column of scores on the continuous predictor for estimating b1 (slope).

Solve for B.
```{r}
B <- ginv(X) %*% Y
B
```

You can check your parameter estimates by running the regression model in `lm` using the original variables (not the matrix versions) and looking at a `summary()` of the results.
```{r}
mental_flex <- c(16, 9, 4, 3, 18)
video_games <- c(8, 5, 2, 0, 10)

model <- lm(mental_flex ~ video_games)
summary(model)
```


A reminder of the definitions of SS_regression, SS_error, and the F-statistic for a regression analysis.

* SS_error is the sum of squares of your error terms
* SS_regression is the sum of squares for your model (sum of squares for the predicted values around the mean of the predicted values, which is the grand mean)
* SS_total = SS_regression + SS_error
  + where SS_total is the total sum of squares on your outcome variable
* F-statistic is the ratio of the variance explained by the model (MS_regression) divided by the error variance (MS_error)

The MS_regression is:
$$MS_{Regression} = \frac{SS_{regression}}{df_{regression}}$$
And the MS_error is:
$$MS_{error} = \frac{SS_{error}}{df_{error}}$$


And the F-statistic is the variance explained by the model, MS_regression divided by the error variance, MS_error.
$$F = \frac{MS_{regression}}{MS_{Error}}$$


You can check your SS, MS, and F by running your model using `lm` and looking at the `anova()` results.


# Interactions/Moderation & Simple Slopes

## Reminders & Tips:

* Specifying interactions between categorical predictors and continuous predictors is the same in lm() as it is for continuous predictors (just `lm(y ~ x1*x2)` regardless of whether x1 and/or x2 are continuous or categorical.
* Make sure to center any continuous variables 
* Make sure categorical variables are read as "factor" variables.
    * you can check this with `str(DATA)` and change something to a factor with `factor()` or `as.factor()` (when manipulating data with `mutate()` or `DATA$Variable = `.

* Significance tests for simple slopes of continuous X continuous interactions require calculating the *SE* 'by hand' (See Lecture 5 slides and Lab 3 R file).
* Significance tests for simple slopes are more straightforward continuous X categorical interactions, where you basically have 3 options:
    1. Use the same procedure/formula as we used in Lab 3
    2. Run your full model twice, changing the reference group for the categorical variable (significance test for slope of continuous predictor in full model is the significance test for the slope when other variables=0, i.e., it is the simple slope for the reference group). _Note: I find this to be the easiest_
    3. Calculate the significance test by hand using MS model from subsetted data and MS error from full model.
    
## Review Questions:

Q: In broad terms, what does an interaction tell us?    
A: *The effect of 1 variable differs across the levels of the other variable.*

Q: Why do we test for simple slopes - what does that tell us?    
A: *It tells us whether the effect of a variable is significant at particular level(s) of the other variable. It helps us understand the interaction (e.g., is the effect of one variable not significant at some level of the other?, etc.)*

It allows us to examine the relationship between number of publications and salary *separately* for males and females.

## Continuous X Categorical Interaction in R.

Let's take a look at a continuous X categorical interaction in R. We'll use the salaries data from Lecture which has professor salaries, data about their work (publications, citations), and their sex (male or female). 
```{r}
salaries <- import("Lecture5Salary.sav")
str(salaries)

salaries <- import("Lecture5Salary.sav") %>% 
  mutate(female = factor(female))

levels(salaries$female) # 0 = male, 1 = female

salaries <- import("Lecture5Salary.sav") %>% 
  mutate(female = factor(female, labels = c("male", "female")))
```

### Regress Salary on (centered) publications and sex.
```{r}
full_model <- lm(salary ~ c_pubs*female,
                 data = salaries)

summary(full_model)
```


### Make a Plot
```{r}
ggplot(data = salaries, aes(x=c_pubs, y=salary, group=female, color=female))+
  geom_point()+
  geom_smooth(method="lm") #to add lines
```

### Test Simple Slopes

Q: *Write the equation for the simple slopes for males and females.*    

Full equation: Salary-hat = b0 + b1 * c_pubs + b2 * female + b3 * c_pubs * female

For males (female = 0): 

Salary-hat = b0 + b1 * c_pubs + b2 * 0 + b3 * c_pubs * 0
= b0 + b1 * c_pubs
y-intercept = b0 = 55649
slope = b1 = 446.80


For females (female = 1):

Salary-hat = b0 + b1 * c_pubs + b2 * 1 + b3 * c_pubs * 1
= b0 + b2 + (b1 + b3) * c_pubs
y-intercept: b0 + b2 = 52855
slope: b1 + b3 = 446 - 350 = 96



```{r}
# If we wanted to change the reference group to females...
DC1 <- c(1, 0)
contrasts(salaries$female) <- DC1

full_model <- lm(salary ~ c_pubs + female + c_pubs*female,
                 data = salaries)

summary(full_model)


# and using the simple_slopes() function
simple_slopes(full_model)
```

# Structural Equation Modeling

## Terminology/Notation:

* A variable is **exogenous** if its causes are not represented in the model - therefore, all exogenous variables are free to vary, which is represented by a curved, double-headed arrow on the variable, like the following:

![Exogenous Arrow](pics/exo_arrow.png)

* A variable is **endogenous** if its causes are represented in the model - these variables have an error term that is free to vary, which is represented by the following symbol:

![Endogenous Error](pics/end_err.png)

* *observed*, *manifest*, or *indicator* variables are represented as rectangles:

![Indicator Variable](pics/indicator.png)

* *latent variables* or *factors* are represented as circles or ovals. Their variances are sometimes called *disturbances*:

![Latent Variable](pics/factor.png)

## Practice drawing models!

Example 1: 
We want to explore executive functioning in preschoolers. We believe that the factors underlying executive functioning in preschoolers can be grouped into three general abilities:  task switching (TS), working memory (WM), and inhibitory control (IC). We want to better understand the relationship between these abilities.

We measure task switching with Day/Night, Grass/Snow, and Reverse Categorization (Dimensional Change Card Sorting Task). 
We measure working memory with Corsi Blocks and Spin the Pots. 
We measure inhibitory control with Bear/Dragon, Whisper, and Gift Delay. 

Q: What are the latent factors?   
A: *Task Switching, Working Memory, & Inhibitory Control*   

Q: What are the indicators?
A: Day/Night, Grass/Snow, Reverse Categorization, Corsi Blocks, Spin the Pots, Bear/Dragon, Whisper, and Gift Delay.

Draw the model:   

```{r}
# This is one way to model it, using semPaths. I'd suggest drawing it by hand first and then checking your work with r.
library(semPlot)
library(lavaan)

observed_vars_names <- c("DayNight", "GrassSnow", "RevCat", "Corsi", "SpinPots", "BearDragon", "Whisper", "GiftDelay")
observed_vars <- lapply(observed_vars_names, function(x) assign(x, rnorm(100)))
observed_vars_df <- as.data.frame(do.call("cbind", observed_vars))
colnames(observed_vars_df) <- observed_vars_names

plot_model <- '
# First, lets write out the paths for each latent factor
TS =~ DayNight + GrassSnow + RevCat # task switching
WM =~ Corsi + SpinPots # working memory
IC =~ BearDragon + Whisper + GiftDelay # inhibitory control

#Next, lets make sure we show the covariances between all the latent factors
TS ~~ WM + IC
WM ~~ IC
'

fit <- cfa(plot_model, data = observed_vars_df)
semPaths(fit, layout = 'tree2', nCharNodes = 6)
```



Q: How many unique observations (variances & covariances) are there?    
A:  *q = 36*  
$q = \frac{(N^{2}+N)}{2}$  *or*  $\frac{N(N+1)}{2}$    
*N = unique variances = 8, because there are 8 observed variables. So in order to get the total number of unique observations, including both variances and covariances, plug 8 into the above equation to get q.*  
  
  
  
Q: How many parameters are we estimating?     
A:  *k = 19: 5 paths, 8 error variances, 3 disturbances/ factor variances, 3 factor covariances.*  



Q: How many degrees of freedom does our model have?   
A:  *df = 17*  
$df = \frac{N{^2}+N-2k}{2}$   


Q: A good bare-minimum rule of thumb is to have 20 observations for each parameter estimated (the N:k rule). What N do we need? (Sorry for the confusing re-use of letters here... Whereas N represented the number of unique (observed) variances in the previous few questions, for this N:k rule, N represents the minimum number of subjects or cases).    
A:  *380*  
*Since the N:k rule requires a minimum subjects-to-parameters ratio of 20:1, we multiplied the number of parameters (19) by 20.*    
  

# Programming in R

## Simulation Exercise 

*A researcher is studying the effectiveness of an intervention aimed at increasing people's levels of conscientiousness. She believes that people who received the intervention will have higher levels of conscientiousness than people who did not receive the intervention. For this exercise, you will estimate the power of her study using a simulation with 1000 iterations.*

*Assume both populations (one hypothetical population comprised of those who received the intervention, the other population comprised of those who did not) are normally distributed with SD=4. Population mean for people who received the intervention is equal to 20, and population mean for people who did not receive the intervention is equal to 18. Simulate 1000 replications of the study. What proportion of p-values show a significant difference between samples 1 and 2 (p less than .05?) Use an independent samples t-test to get the p-value for each iteration.*

```{r}
# set.seed if you want the results to be reproducable
set.seed(12093)
# Create storage vector of NA's to store pvalues
p_vals <- rep(NA, 1000)

# Tell R to run 1000 iterations of the study

iter <- 1000

# Start the for loop

for (i in 1:iter) {

  # Randomly select 2 samples from normal populations 
  # with the characteristics described above (hint: see ?rnorm)

  sample_1 <- rnorm(n = 30, mean = 20, sd = 4)
  
  sample_2 <- rnorm(n = 30, mean = 18, sd = 4)

  # Perform t test on your samples 
  # (Hint: save as object so you can extract pvalue. 
  # You can use t.test(x=sample1data, y=sample2data, alternative="two.sided", var.equal=TRUE))
  
  results <- t.test(x = sample_1, y = sample_2, 
                    alternative = "two.sided", var.equal = TRUE)

  # Put pvalue into appropriate place in storage vector
  p_vals[i] <- results$p.value
  
# End for loop
}
  
# Calculate proportion of pvalues < .05 
# (Hint: use ifelse() to create a new vector 
# indicating which are <.05, then use table() to count frequencies)

sig <- ifelse(p_vals > .05, "NonSig",
              ifelse(p_vals <= .05, "Sig", NA))

sig_table <- table(sig)

sig_table[[2]] / (sig_table[[1]] + sig_table[[2]])
```

Q: Based on this simulation, what is the estimated power of this study? You can check how close it is using the pwr package in R.

A: *.48*

Next, we can see if that is the same as what traditional power calculations would tell us:

```{r}
library(pwr)
pwr.t.test(n = 30, d = .5, sig.level = .05, type = "two.sample", alternative = "two.sided")
```