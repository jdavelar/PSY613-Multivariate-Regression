---
title: "613 Lab 3: Interactions & Simple Slopes"
output: 
  html_document: 
    fig_caption: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r libs, message = FALSE, warning = FALSE}
library(rio)
library(tidyverse)
library(broom)
# install.packages("reghelper")
library(reghelper)
library(here)
library(psych)

options(scipen=999)
```

# Interactions in Multiple Regression

* Interactions between continuous predictors
* Interactions between categorical predictors
* Interactions between continuous and categorical predictors

Today, we'll be looking at an interaction between two continuous predictors.

## Research Scenario

_For her masters project, a grad student wants to see whether the time students spend studying (`time`) interacts with test anxiety (`anx`) to predict students' test performance (`perf`)._

### Import the Data
```{r}
simple_slopes_df <- import(here("data", "613_Lab03_simple_slopes.sav")) %>% 
  janitor::clean_names()
```

Let's take a look at the data that we imported from an SPSS file.
```{r}
head(simple_slopes_df)
str(simple_slopes_df)
describe(simple_slopes_df)
```


Q: What is the outcome variable (i.e., DV)?   
A: Test performance

Q: What are the predictors (i.e., X1 and X2)?    
A: Test anxiety and time spent studying; also the interaction between anxiety and time

Q: Should we center the predictors before computing the interaction term? If so, which variables?   
A: Yes, we want to center because it aids in interpretability and avoids issues of multicollinearity. We want to center both predictor variables (time and anxiety), relative to the mean. In other words, we'll replace "0" with the mean values for predictors so that the y-intercept is more meaningful.

Q: How would you write the regression equation for this scenario? 
A: $Y_{performance} = \beta_0 + \beta_{1 anxiety} + \beta_{2 time} + \beta_{3 anxiety*time}$

What we're using statistics to find is each of the parameters $\beta$

## Step 1: Center Predictors

To create mean-centered predictors, we'll take the raw values and subtract the mean from them.

Using the `mutate` function, we can add the new centered versions of each predictor to our dataset. 
```{r}
simple_slopes_df <- simple_slopes_df %>% 
  mutate(anx_c = anx - mean(anx, na.rm = TRUE),
         time_c = time - mean(time, na.rm = TRUE))
head(simple_slopes_df)
```

Our new columns show how far away each subject was from the mean. For example, using `head()` we can see that subject 1 had an anxiety score that was 2.068... *below* the mean.

And, we can make sure our centering worked.

```{r}
simple_slopes_df %>% 
  summarize(m_anx_c = mean(anx_c, na.rm = TRUE),
            m_time_c = mean(time_c, na.rm = TRUE)) %>% 
  round(5)
```

Yep, looks like it worked; the mean of both variables is zero.

## Step 2: Model the Interaction

Next, we'll model the interaction. There are a few ways to do this, but the easiest way to do is to simply run an `lm()` and enter our two variables separated by a `*`.

```{r}
anx_x_time_model <- lm(perf ~  time_c * anx_c, data = simple_slopes_df) # the main effects are automatically included when you include the interaction between two variables

## In other words, the above equation is equivalent to:
anx_x_time_model2 <- lm(perf ~ time_c + anx_c + time_c * anx_c, data = simple_slopes_df)
```

### Look at Model Results
```{r}
summary(anx_x_time_model)
```

You can also make a nice parameter table using `broom::tidy()`
```{r}
param_table <- anx_x_time_model %>% 
  tidy(conf.int = TRUE) %>% 
  janitor::clean_names()

knitr::kable(param_table, 
             caption = "Parameter Table Predicting Performance from Anxiety X Time",
             digits = c(NA, 2, 2, 2, 5, 2, 2))
```


Q: Does the effect of time spent studying depend on one's standing on anxiety?   
A: Yes, the relationship between time studying and test performance varies significantly depending on on people's level of anxiety.

Q: What does the intercept mean (using the value in your answer)?
A: $\beta0$ = 77.82 
Predicted test performance for someone who scores equal to the mean on time spent studying and mean on anxiety levels. (Average anxiety and average time studying = predict a score of 77.82 on exam.)

Q: What about the estimate for anxiety?   
A: $\beta2$ = -2.73 
Test scores are predicted to decrease by 2.73 points for every 1-unit increase in anxiety holding time spent studying constant at the mean (rather than 0, because we centered our predictor variables).

Q: What about the estimate for time?   
A: $\beta1$ = 5.85
Our model predicts that test performance scores will increase by 5.86 for every unit increase in time spent studying given an average score of anxiety.

Q: What about the time X anxiety interaction?
A: $\beta3$ = -2.33
The easiest way to understand the interaction is by plotting it, which we'll do below.

*Notice*: We centered but did not standardize our variables, that means the standard unit increases we describe above is still in terms of the raw data units, not in terms of standard deviation units. If we want to standardize (by converting to z-score) then we can use `scale()` function which does that for us. Default standardizes to z-score, but you can also set second argument to `FALSE` to *only* center but not standardize if you want to interpret according to SD units.

## Step 3: Plot the Interaction

Next we'll unpack what the interaction means by plotting simple slopes. Let's specifically examine the question of whether the relationship between time and test performance varies depending on level of anxiety (you could also examine whether the relationship between anxiety and test performance varies depending on level of time).

Specifically, let's get simple slopes for time at different (average) levels of anxiety. To do this, we'll plot a line for the relationship between time and test performance for people high in anxiety (i.e., 1 SD above the mean), average levels of anxiety (i.e., at the mean), and low in anxiety (i.e., 1 SD below the mean).

### Calculate Simple Intercept & Slope *Leading into our Simple Slope Analysis*

#### Step 3a: Extract Regression Coefficients from the Model

You can find the model coefficients stored in the `coefficients` component of the model output.
```{r}
anx_x_time_model$coefficients

# How do we get the individual values? INDEXING!

b0 <- anx_x_time_model$coefficients[1]
b_time <- anx_x_time_model$coefficients[2] # b1
b_anx <- anx_x_time_model$coefficients[3] # b2
b_anx_x_time <- anx_x_time_model$coefficients[4] # b3

# Make sure the correct values were stored

b0
b_time
b_anx
b_anx_x_time
```


#### Step 3b: Calculate Intercepts and Simple Slopes of Time
Next, we'll calculate the intercepts and time slopes for individuals at low, middle, and high levels of anxiety. Before we do that, we'll need to get low, medium, and high levels of anxiety, which is generally 1 SD below the mean, the mean, and 1 SD above the mean respectively.

##### Get Low, Mid, & High values of Anxiety
```{r}
sd(simple_slopes_df$anx_c)

low_anx <- mean(simple_slopes_df$anx_c, na.rm = TRUE) - sd(simple_slopes_df$anx_c, na.rm = TRUE) 
mid_anx <- mean(simple_slopes_df$anx_c, na.rm = TRUE)
high_anx <-  mean(simple_slopes_df$anx_c, na.rm = TRUE) + sd(simple_slopes_df$anx_c, na.rm = TRUE)
```

The full regression equation is:
* y = b0 + b1 * time_c + b2 * anx_c + b3 * (time_c*anx_c)
* y = b0 + b_time * time_c + b_anx * anx_c + b_anx_x_time * (time_c * anx_c)

We'll be plugging in specific values for anx_c (-1SD, mean, +1SD) to solve for the simple slopes between time and performance at each level of anxiety.

The intercepts for the simple slopes of time are:
* y = b0 + b2 * anx_level
* y = b0 + b_anx * anx_level 

And the slopes for the simple slopes of time are:
* y = b1 + (b3 * anx_level)
* y = b_time + (b_anx_x_time * anx_level)

where you replace anx_level with low_anx, mid_anx, or high_anx


##### Calculate The simple slopes and intercepts
```{r}
intercepts <- c(b0 + (b_anx*low_anx), 
                b0 + (b_anx*mid_anx), 
                b0 + (b_anx*high_anx)) 

slopes <- c(b_time + (b_anx_x_time*low_anx),
            b_time + (b_anx_x_time*mid_anx), 
            b_time + (b_anx_x_time*high_anx))

# create the level & column labels to make things easy to check
anx_levels <- c("low","mid","high") 

# put them all together into a dataframe
int_plot_df <- data.frame(anx_levels, 
                         intercepts,
                         slopes)
int_plot_df
```

##### Step 3c: Plot!

Finally, we'll make the plot using `ggplot` and a few different `geoms`, including `geom_point()`, `geom_abline()`
```{r}
ggplot(simple_slopes_df, aes(x = time_c, y = perf)) + # time on X, performance on Y
  geom_point(stat = "identity", shape = 1, size = 2) +  # add points for scatterplot
  # next we'll add the lines for each level of support, using geom_abline (you specify intercept and slope)
  geom_abline(aes(intercept = int_plot_df$intercepts[1], slope = int_plot_df$slopes[1], 
                  colour = "-1SD Anxiety"), size = 1) +
  geom_abline(aes(intercept = int_plot_df$intercepts[2], slope = int_plot_df$slopes[2], 
                  colour = "Mean Anxiety"), size = 1) + 
  geom_abline(aes(intercept = int_plot_df$intercepts[3], slope = int_plot_df$slopes[3], 
                  colour = "+1SD Anxiety"), size = 1) +
  scale_color_manual(values = c("-1SD Anxiety" = 'cornflower blue', "Mean Anxiety" = 'orchid', "+1SD Anxiety" = 'salmon')) +
  labs(title = "Effect of Time Spent Studying at Low, Medium, and High Test Anxiety", 
       x = "Time (Centered)", y = "Test Performance", colour = "Anxiety Level")
```


Q: What do you make of the interaction?    
A: As anxiety increases, the effect of time spent studying weakens.

## Step 4: Testing Significance of Simple Slope

We don't have a way of testing the significance of our simple slopes generally, so we'll do it manually by constructing a t-test for each slope (low, average, and high anxiety).

Q: Which of the simple slopes does it make sense to test first (in this example)?
A: The low anxiety level seems very steep, so we'll start there.


Let's test the significance of the simple slopes of time spent studying for people with low test anxiety. Recall from lecture that to test a simple slope, we have to calculate the standard error for the simple slope. The Standard error for a simple slope is:

$$SE_{b\_at\_x2} = \sqrt{SE^2_{b1} + 2 * X2(cov(b1, b3)) + X2^2SE^2_{b3}}$$

Which we need to calculate the *t* for a simple slope, which is:

$$t_{b\_at\_X2} = \frac{b_1 + b_3X2}{SE_{b\_at\_x2}}$$
For this example:
* b1 = b_time
* b3 = b_anx_x_time
* X2 = anx_c

### Calculate Standard Error
In order to calculate this, we  need to get each slope's variance and the covariance between slopes. As Elliot mentioned in lecture, we can get this in R from the `vcov()` function.

```{r}
vcov(anx_x_time_model)
```

Recall that, just like a variance-covariance matrix of variables, the principal diagonal contains variances and covariances are on the off-diagonal (symmetrical about the diagonal). We'll extract the values we need and save them into objects. Note that we can do this either positionally (i.e., the variance for time_c can be retrieved with `vcov(anx_x_time_model)[2,2]`) or by using the row and column names (i.e., the variance for time_c can be retrieved with `vcov(anx_x_time_model)["time_c", "time_c"]`)

```{r}
se_b1_sq <- vcov(anx_x_time_model)["time_c", "time_c"]
se_b3_sq <- vcov(anx_x_time_model)["time_c:anx_c", "time_c:anx_c"]
cov_b1_b3 <- vcov(anx_x_time_model)["time_c", "time_c:anx_c"]
```

And you might want to verify that it looks right:

```{r}
se_b1_sq
se_b3_sq
cov_b1_b3
```

And then we just put in the formula we wrote out above and plug in these values to calculate our Standard Error.
```{r}
se_b_time_low_anx <- sqrt(se_b1_sq + 2*low_anx*cov_b1_b3 + (low_anx)^2*se_b3_sq)
se_b_time_low_anx #use this for our denom in calculating t-stat
```

### Calculate t stat

Next, we'll divide the slope for time at low levels of anxiety (that we calculated earlier) by its SE to get a t value. Note that this is *t* distributed with $df = N - k - 1$ *(where k is the number of variables)*
```{r}
int_plot_df[1, 3] # this is where the value of b1 + b3*low_anx is stored

t_b_time_low_anx <- int_plot_df[1, 3] / se_b_time_low_anx

t_b_time_low_anx
```

Next, we'll get the p value associated with this t value. First we need the *df*:

```{r}
df <- nrow(simple_slopes_df) - 4 # N - number of parameters (b0, b1, b2, b3)
```

And finally we'll get the p value:
```{r}
# pt() provides the probabilities of lying anywhere on the t-distribution 

2*pt(t_b_time_low_anx, df, lower.tail = FALSE) # multiplied by 2 because we typically perform two-tailed tests
```

Q: Is the simple slope of time spent studying at low levels of test anxiety significant?
A: Yes, the relationship between time spent studying and test performance is significant at low levels of test anxiety, b = 8.90, t(69) = 11.01, p < .001.


### Perfoming Simple Slopes Analysis Using the simple_slopes() function

The `simple_slopes()` function from the `reghelper` package can also perform the simple slopes analysis calculations that we just did by hand. 

This function tests the simple effects of the highest-order interaction in the regression equation. If there are multiple interactions of the highest order, it tests the simple effects of the interaction term that comes first. Meaning, you may have to rearrange your model if you have multiple interactions and the one you want to test is not listed first.

```{r}
simple_slopes(anx_x_time_model)
```

Lines 4-6 in the output is measuring the simple slopes at low (4), medium (5), and high anxiety (6). `Estimate` gives you the slope, `Std. Error` gives you the slopes, `t value` is your t-statistic, and of course, you also get the df and p-value without having to calculate everything by hand. We can see (much more quickly than calculating by hand) that the relationship between time spent studying and test performance is significant at all 3 levels of test anxiety (low, medium, high). 

Lines 1-3 do the same but for time spent studying if you were analyzing from time rather than anxiety.

# Importance of Centering

Let's take a quick look at how centering affects the collinearity of our predictors. 

First, you can see that  by creating an interaction term and getting the correlations between the variables. Let's calculate those interaction terms:

```{r}
simple_slopes_df <- simple_slopes_df %>% 
  mutate(time_x_anx = time*anx,
         time_x_anx_c = time_c*anx_c)
```

Then look at the correlation:

```{r}
simple_slopes_df %>% 
  select(anx, time, time_x_anx) %>% 
  cor()
```

Q: What do you notice?
A: Correlation between interaction and each variable (anx .67; time .75) is *very* high. We should be concerned about multicollinearity.

Now let's take a look at it for the centered variables:
```{r}
simple_slopes_df %>% 
  select(anx_c, time_c, time_x_anx_c) %>% 
  cor()
```

With our centered variables, the correlatin is much lower.

You can also see this in action by taking a look at the VIF of a model with and without centering

```{r}
car::vif(lm(perf ~ time * anx, data = simple_slopes_df))
```

And let's see it for the centered model.

```{r}
car::vif(anx_x_time_model)
```

Ahh, much better!

