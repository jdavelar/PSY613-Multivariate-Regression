---
title: "Factor Analysis"
author: "Janette Avelar (adapted from Rachael Smith and Andrew Johnson)"
date: "4/18/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library}
library(psych)
library(GPArotation)
```

# Introduction

For this lab, we are going to explore the factor analysis technique, looking at both principal axis and principal
components extraction methods, two different methods of identifying the correct number of factors to extract
(scree plot and parallel analysis), and two different methods of rotating factors to facilitate interpretation.

# Data

The dataset that we’ll use for this demonstration is called `bfi` and comes from the `{psych}` package. It is made
up of 25 self-report personality items from the International Personality Item Pool, gender, education level
and age for 2800 subjects and used in the Synthetic Aperture Personality Assessment.
The personality items are split into 5 categories: Agreeableness (A), Conscientiousness (C), Extraversion
(E), Neuroticism (N), Openness (O). Each item was answered on a six point scale: 1 Very Inaccurate, 2
Moderately Inaccurate, 3 Slightly Inaccurate, 4 Slightly Accurate, 5 Moderately Accurate, 6 Very Accurate.

Here's how to load it:

```{r load data}
data("bfi")
```

## Describing the Data

It’s a good idea to look at your data before you run any analysis. Any participant who is missing any piece
of data will be fully excluded from the analysis. It’s important to keep that in mind.

```{r describe data}
#all data
describe(bfi)
#look at just cols 1-25
describe(bfi[1:25])

#notice some are missing... figure out who
sum(complete.cases(bfi[1:25]))
```

This suggests that most items are only missing 20 or 30 participants worth of data - which is no big deal in a
data set with 2800 observations. It is possible, however, that some of these missing values are non-overlapping
- meaning that it could be a different 20 or 30 individuals missing from each of the variables. We can, however,
determine the number of “complete cases” within the data - these are individuals that are missing no data
whatsoever on the questionnaire.

The `complete.cases()` function generates a Boolean vector, where a value of “TRUE” means that the case is
complete, and a value of “FALSE” means that the case is missing at least one value. Summing across this
vector gives us the total number of complete cases. This means that there are 2436 cases with no missing
data. This means that 13% of the data is missing. There is no magic number as to the amount of missing
data that is acceptable - but sample size is important for factor analysis. Some authors suggest that you
need at least 10 observations for each variable in the factor analysis - our sample size is, therefore, adequate
for our purposes.

# Assessing the Factorability of the Data

Before we go too far down the road with this analysis, we should evaluate the “factorability” of our data. In
other words, “are there meaningful latent factors to be found within the data?”  

We can check two things:   
(1) Bartlett’s test of sphericity; and  
(2) the Kaiser-Meyer-Olkin measure of sampling adequacy.

## Bartlett's Test of Sphericity

I think we used this in EDUC 642, or a different test of sphericity, but it may be worth going back to those 
notes to see if there's more information you can get there.

The most liberal test is Bartlett’s test of sphericity - this evaluates whether or not the variables intercorrelate
at all, by evaluating the observed correlation matrix against an “identity matrix” (a matrix with ones along
the principal diagonal, and zeroes everywhere else). *If this test is not statistically significant, you should not
employ a factor analysis.*

Here's how you run it:

```{r test of sphericity}
#note that the original document runs through [1:25] index
#but doesn't explain why; not sure why we're doing that
cortest.bartlett(bfi[1:25])
```

Bartlett’s test was statistically significant, suggesting that the observed correlation matrix among the items is
not an identity matrix. This really isn’t a particularly powerful indication that you have a factorable dataset,
though - all it really tells you is that at least some of the variables are correlated with each other.

## KMO

The Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy is a better measure of factorability. The
KMO tests to see if the partial correlations within your data are close enough to zero to suggest that there is
at least one latent factor underlying your variables. The minimum acceptable value is 0.50, but most authors
recommend a value of at 0.60 before undertaking a factor analysis. The `KMO()` function in the psych package
produces an overall Measure of Sampling Adequacy (MSA, as its labelled in the output), and an MSA for
each item. Theoretically, if your overall MSA is too low, you could look at the item MSA’s and drop items
that are too low. This should be done with caution, of course, as is the case with any atheoretical, empirical
method of item selection.

```{r KMO}
KMO(bfi[1:25])
```

The overall KMO for our data is 0.85 which is excellent - this suggests that we can go ahead with our planned
factor analysis.

# Determining the Number of Factors to Extract

The first decision that we will face in our factor analysis is the decision as to the number of factors that we
will need to extract, in order to achieve the most parsimonious (but still interpretatable) factor structure.
There are a number of methods that we could use, but the two most commonly employed methods are the
scree plot, and parallel analysis. The simplest technique is the scree plot.

## Scree Plot

Eigenvalues are a measure of the amount of variance accounted for by a factor, and so they can be useful in
determining the number of factors that we need to extract. In a scree plot, we simply plot the eigenvalues for
all of our factors, and then look to see where they drop off sharply.

Let’s take a look at the scree plot for the bfi dataset:

```{r scree}
scree(bfi[1:25])
```

The scree plot technique involves drawing a straight line through the plotted eigenvalues, starting with the
largest one. The last point to fall on this line represents the last factor that you extract, with the idea being
that beyond this, the amount of additional variance explained is non-meaningful. In fact, the word “scree”
refers to the loose stone that lies around the base of the mountain. A “scree plot” is effectively looking to
help you differentiate between the points that represent “mountain”, and the points that represent “scree.”

Regardless of whether you are using a principal components or a principal axis factor extraction, however,
there is a very large first factor in this data. If we were to draw our straight line starting at this point, you
would probably conclude that there are only three factors in the dataset. If, however, you were to start your
line at the second point in the scree plot, you would probably conclude that there are five factors in the
dataset. The latter interpretation is probably closer to the truth, but if this were the only piece of evidence
brought to bear in our consideration of the number of factors to extract, we might want to look at both of
these factor solutions.

The above explanation (2 paragraphs) is from original document, and I do not understand. Wait until class for explanation.

## Parallel Analysis

A better method for evaluating the scree plot is within a parallel analysis. In addition to plotting the
eigenvalues from our factor analysis (whether it’s based on principal axis or principal components extraction),
a parallel analysis involves generating random correlation matrices and after factor analyzing them, comparing
the resulting eigenvalues to the eigenvalues of the observed data. The idea behind this method is that
observed eigenvalues that are higher than their corresponding random eigenvalues are more likely to be from
“meaningful factors” than observed eigenvalues that are below their corresponding random eigenvalue.

We'll use `fa.parallel()` to run the parallel analysis:

```{r parallel analysis}
fa.parallel(bfi[1:25])
```

When looking at the parallel analysis scree plots, there are two places to look depending on which type of
factor analysis you’re looking to run. The two blue lines show you the observed eigenvalues - they should
look identical to the scree plots drawn by the scree function. The red dotted lines show you the random
eigenvalues or the simulated data line. Each point on the blue line that lies above the corresponding simulated
data line is a factor or component to extract. In this analysis, you can see that 6 factors in the “Factor
Analysis” parallel analysis lie above the corresponding simulated data line and 6 components in the “Principal
Components” parallel analysis lie above the corresponding simulated data line.

In our case, however, the last factor/component lies very close to the line - for both principal components
extraction and principal axis extraction. Thus, we should probably compare the 6 factor and 5 factor solutions,
to see which one is most interpretable.


# Conducting the Factor Analysis

We already have a good idea as to how many factors (5 or 6) that we should extract in our analysis of the bfi
data object. Now we need to decide whether we will use “common factor” analysis, or “principal components”
analysis. In a very broad sense, “common factor” analysis (or “principal axis factoring”) is used when we
want to identify the latent variables that are underlying a set of variables, while “principal components”
analysis is used to reduce a set of variables to a smaller set of factors (i.e., the “principal components” of the
data). In other words, common factor analysis is used when you want to evaluate a theoretical model with a
set of variables, and principal components analysis is used for data reduction.

Both of these approaches have merit in test construction, and so we will walk through each approach with
this data.

## Principal Axis Factoring (Common Factor Analysis)

The `fa()` function takes the following parameters:  
* the variables to be used within the factor analysis (items 1-25 from `bfi`)  
* the number of factors we want to extract (6)  
* the type of factor analysis we want to use (`pa` is principal axis factoring)  
* the number of iterations or attempts to use when identifying the "best" solution  
(50 is the default, but we'll change it to 100)  
* the type of rotation we want to use (we'll start with `oblimin`)

Our argument will look like this:

```{r CFA x6}
#pa6.out is the object name in the original document
#why? pa data loading onto 6 factors
pa6.out <- fa(bfi[1:25],
              nfactors = 6,
              fm = "pa",
              max.iter = 100,
              rotate = "oblimin")
```

Notice this won't generate any output.

A quick way to visualize your rotated factor solution, and determine whether it represents an "interpretable" 
solution is to use `fa.diagram()`.

```{r CFA x6 viz}
fa.diagram(pa6.out)
```

As you can see from this, the sixth factor has only one variable loading on it - the second item on the openness
to experience scale. Thus, this probably represents an overextraction. . . let’s take a look at the five factor
solution.

```{r CFA x5}
pa5.out <- fac(bfi[1:25],
               nfactors = 5,
               fm = "pa",
               max.iter = 100,
               rotate = "oblimin")
fa.diagram(pa5.out)
```

The five-factor solution is more interpretable - in fact, it seems to replicate the expected factor structure
nicely.

## Communalities

The communality for each variable is the percentage of variance that can be explained by the retained factors.
It’s best if the retained factors explain more of the variance in each variable.

Here's how to extract:

```{r CFA communalities}
pa5.out$communality
```

As a point of interest, the primary difference between the way that common factor analysis and principal
component analysis are conducted, is that the correlation matrix on which the factor analysis is based has
ones along the principal diagonal in principal components analysis, and the communalities along the principal
diagonal in principal axis factor analysis.

## Eigenvalues

The eigenvalues derived in the extracted factor solution are stored within `e.values`. These are the eigenvalues
that were plotted in the scree plots that we looked at near the beginning of this process.

Here's how to extract:

```{r CFA eigens}
pa5.out$e.values[1:5]
```

If you want the eigenvalues from the rotated solution, you would ask for values:

```{r CFA rotated eigens}
pa5.out$values[1:5]
```

## Percentage of Variance Accounted For

We can use the eigenvalues to calculate the percentage of variance accounted for by each of the factors. Given
that the maximum sum of the eigenvalues will always be equal to the total number of variables in the analysis,
we can calculate the percentage of variance accounted for by dividing each eigenvalue by the total number of
variables in the analysis. In our example this is 25.

```{r CFA variance}
100*pa5.out$e.values[1:5]/length(pa5.out$e.values)
```

If you wanted the percentage of variance accounted for by the rotated solution, you would use the eigenvalues
stored in `values` rather than `e.values`.

```{r CFA rotated variance}
100*pa5.out$values[1:5]/length(pa5.out$values)
```

## Rotated Solution

We’ve already peeked at the highest-loading items for each factor (using `fa.diagram`), but this only tells us
the largest loading for each item. Each item will, however, load on each of the factors to a greater or lesser
degree - and we will eventually want to look at the full factor loading matrix. The factor loading matrix
shows us the factor loadings for each variable, after they have been rotated to “simple structure.” Essentially,
we are taking advantage of the fact that there are a number of factor solutions that are equally acceptable to
the “optimal” solution that was found within our initial extraction (i.e., that are mathematically equivalent),
and rotating the factors so that they are more easily interpreted.

Because we have used an oblique factor rotation (“oblimin”), this is sometimes (e.g., in SPSS) called a pattern
matrix.

```{r CFA pattern matrix}
print(pa5.out$loadings, cutoff = 0, digits = 3)
```

We can also look at the structure matrix - this is just the pattern matrix multiplied by the factor intercorrelation
matrix. The result is that these values represent the correlations between the variables and the factors -
which may be more intuitive to interpret.

```{r CFA structure matrix}
print(pa5.out$Structure, cutoff = 0, digits = 3)
```


# Principal Components Analysis

This is how you run a Principal Components Analysis in R. The command is not the same as running
Principal Axis Factoring. Many of the steps will be the same, but we’ll go through them for the Principal
Components Analysis as well.

```{r PCA x6}
pc6.out <- principal(bfi[1:25],
                     nfactors = 6,
                     rotate = "oblimin")
```

Again, we can take a quick look at the factor structure for this solution using `fa.diagram()`.

```{r PCA x6 viz}
fa.diagram(pc6.out)
```

As you can see, the six factor solution results in a factor that has major loadings from only two items, and is
not easily interpreted. Let’s take a look at the five factor solution.

```{r PCA x5}
pc5.out <- principal(bfi[1:25], 
                     nfactors = 5,
                     rotate = "oblimin")

fa.diagram(pc5.out)
```

As was the case with the principal axis / common factor solution, the five-factor principal components solution
is far more interpretable, aligning very nicely with the expected item-factor orientation.

Now that we have determined the number of factors to extract, we can look at all of the same information
that we pulled out of the common factor solution.

## Communalities

Again, these are the percentage of variance that can be explained by the retained factors for each variable.

```{r PCA communalities}
pc5.out$communality
```

## Eigenvalues and Percentage of Variance Accounted For

This computes a vector of the eigenvalues for our five principal components:

```{r PCA eigens}
pc5.out$values[1:5]
```

and this uses those eigenvalues to compute the percentage of variance associated with each of these factors:

```{r PCA variance}
100*pc5.out$values[1:5]/length(pc5.out$values)
```

## Rotated Solution

This factor loading matrix shows us the variables that load on each of the factors we have extracted...

```{r PCA pattern matrix}
print(pc5.out$loadings, cutoff=0, digits=3)
```

...and the structure matrix shows us the correlations between the variables and the factors.

```{r PCA structure matrix}
print(pc5.out$Structure, cutoff=0, digits=3)
```

# Final Thoughts

Remember that factor analysis, despite being a mathematically intensive procedure, is highly subjective. You
have to make choices about:  

* the type of extraction method to use (principal axis or principal components)  
* the number of factors to extract  
* the factor rotation method to use when looking for simple structure (and interpretability)  
* the interpretation of the factors  

*Furthermore* the outcome of your factor analysis (and the resulting interpretation of the factors) will be
highly dependent upon the variables that you select for inclusion in the analysis. Factor analysis is an
excellent method for evaluating underlying latent constructs, but... at it’s core, it is simply a method of
parsing large correlation matrices. If you select several variables that are highly related, you should not be
surprised if they group together to form a factor!