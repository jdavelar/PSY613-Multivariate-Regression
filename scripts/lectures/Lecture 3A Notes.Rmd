---
title: "Lecture 2B"
author: "Janette Avelar"
date: "4/7/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(pracma)
library(car)
library(MASS)
```

# Week 2 - 4/7/22

*Programming in R Part II: Matrices in R*

## Nested for loop

We're going to build on what we did last time

```{r example nested for loop}
# see Lecture 3 Rmd file for everything preceding
number_of_sets = c(5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000)

for(set in 1:length(number_of_sets)) {
  
  storage = vector()
  for(samples in 1:number_of_sets[set]) {
    one_try = sample(0:1, 10, replace = TRUE)
    nHeads = sum(one_try)
    storage[samples] = nHeads
  }
}
```

When do you use length, vs the variable itself?

In the example above, we used length. Using number_of_sets instead would run through without counting which time we're on rather than running through the loop a certain set of times.

Advice: start with the inner loop when doing nested loops

## Example Simulation 2

Another thing you can do using basic logic operators is to run permutation simulation to empirically derive the null sampling distribution of a test statistic. 

Group 1: 1, 2, 3, 4
Group 2: 5, 6, 7, 8

Now, suppose we didn't know what a t-test is and so had no idea what the t-distibution looked like. How could we test the difference between these two groups?

Start by remembering that this situation has two variables, a DV (numbers 1:8) and an IV (binary group assignment):

```{r data into a df}
df = data.frame(DV = 1:8, IV = c(0, 0, 0, 0, 1, 1, 1, 1))
df
```
We can estimate the difference between the two groups by comparing the means:

```{r comparing groups}
group_diff = mean(df$DV[df$IV==1]) - mean(df$DV[df$IV==0])
group_diff
```

Under the null, we are assuming a world where there is no difference between the groups. This poses the consequence that we could draw any numbers randomly from the groups we have. So we can use `sample()` to draw random samples to test this out.

Under the null hypothesis, there is no difference between the groups _because they are drawn from the same distribution_. That means that -- under the null -- the binary IV is meaningless. Which means that we can freely shuffle the assignment accordingly. We can use the `sample` function to accomplish this:

```{r random permutation}
permutation_of_DV = sample(df$DV, replace=TRUE)
permutation_of_DV
```

Once we have that, we can repeat the group comparison, this time with a shuffled IV, which we can think of as an independent variable drawn from the null distribtion (where there is no true difference between the groups):

```{r comparing shuffledroups}
group_diff = mean(permutation_of_DV[df$IV==1]) - mean(permutation_of_DV[df$IV==0])
group_diff
```

Of course, that was just _one_ random permutation. 

What would happen if we kept drawing random samples? We'd eventually get closer to 0, because the difference between them is 0 (under the null).

Suppose we wanted to do that a bunch of times, each time recording the group difference. If we did that enough times, we could draw a _sampling distribution_ of the mean difference under the null hypothesis.

Some pointers: Figure out how to write your code to do that once:

```{r}
  permutation_of_DV = sample(df$DV, replace=TRUE)
  group_diff = mean(permutation_of_DV[df$IV==1]) - mean(permutation_of_DV[df$IV==0])
  mean_diff_storage[permutation] = group_diff
```

Then stick it in a for loop to run multiple times. And since you want to save it, you'll also want to create a storage vector:

```{r null sampling distribution}
mean_diff_storage = vector()
n_permutes = 100000 #can change without affecting for loop
  
for (permutation in 1:n_permutes) { #making it easy to change times by assigning to n_permutes
  permutation_of_DV = sample(df$DV, replace=TRUE)
  group_diff = mean(permutation_of_DV[df$IV==1]) - mean(permutation_of_DV[df$IV==0])
  mean_diff_storage[permutation] = group_diff
}

hist(mean_diff_storage)
# as expected, mean difference is 0, but actual mean difference is 4
```

Our actual mean difference is different from the mean difference we generated if the groups were the same. You can think of this as indicating where on the distribution we lie, from which we can gather a p-value that gives us the probability of getting that mean by chance.

Thus, we might want to add a nice vertical line showing where our observed sample difference is and we can calculate its percentile:

```{r observed-vs-null-dist}
hist(mean_diff_storage)
group_diff = mean(df$DV[df$IV==1]) - mean(df$DV[df$IV==0])
abline(v = group_diff, col="red", lwd=3, lty=2)

# Calculate percentile
n_greater = sum(mean_diff_storage>=group_diff)
percentile = n_greater / n_permutes

print(paste0("The probability of drawing two groups with the observed difference or larger is ", percentile))

```

## How do you run the GLM using matrix algebra? [Lecture 4]

```{r}
library(matlab)
library(ggplot2)
```

The first part of the document walks through doing some basic operators, like assigning and creating variables. 

Also includes `seq()` function, which is kind of similar to 1:10 but more flexible. Allows you to assign a variable 1:10 or 10:1 counting by various things (by 1, backwards by 1, by .5, etc.).

Also introduces binding things together using `cbind()` or `rbind()` which stands for combining by rows or columns to bind vectors together. You can also take lists and covert them to a matrix.

```{r matrices}

# can bind things together
x <- 1:10
y <- 11:20
z <- c(x,y) # combines them into one vector
z1 <- cbind(x,y) # combines them into two columns in a matrix
z2 <- rbind(x,y) # combines them into two rows in a matrix

# ...speaking of matrices
data_for_matrix <- c(4,7,8)
x <- as.matrix(data_for_matrix)

# creating matrices
x <- matrix(c(11,12,13,21,22,23), byrow=TRUE,nrow=2) # put into a matrix by row
x1 <- matrix(c(11,12,13,21,22,23), byrow=FALSE,nrow=2) # put into a matrix by column
```

Indexing matrixes: getting the value that's in a particular row/column
x[r,c] would give you the element in row r, column c of the matrix

```{r} 
x[2,3] # gives you the element in row 2 column 3 of matrix x (defined above)
x[] # all rows, all columns - could have just typed x
x[1,] # first row, all columns
x[ ,3] # all rows, third column
x[1,3] # first row, third column
 
# Useful shortcuts - all from {matlab}
x <- ones(5) # gives you a 5x5 matrix of ones
x

x <- zeros(3) # gives you a 3x3 matrix of zeros
x

x <- eye(4) # 4x4 identity matrix
x

rando = matrix(rnorm(16), nrow = 4)
rando

x %*% rando

x <- ones(5,4) # if you don't want a square matrix, can specify rows & columns
x

#here's another way to do the same thing:
x <- matrix(rep(1,20),ncol=4) #rep() is saying repeat '1' 20 times

#let's make x a different matrix
x <- matrix( c(rep(1,5), rep(0,10), rep(1,5))  , byrow=FALSE, ncol=2)
x
```

You can also do actual matrix algebra in R:

```{r}
# ginv = "generalized" or "pseudo" inverse
# pseudo-inverse = (X'X)^-1 * X'

ginv(x) # gives you the pseudo inverse of x
check_ginv = inv( t(x) %*% x ) %*%  t(x)  # check that ginv actually calculates the pseudoinverse

t(x) # gives the transpose of x

size(x) # gives you the dimensions of x (the size function can only be used if you have the pracma or matlab packages open)

df_total = size(x)[1] - 1

dim(x) # alternative way to get dimensions (this is in base R and can be used w/out packages); same as "size()"

rref(x) # only one piece of unique information in this matrix (not surprisingly since the rows are identical!)
```

Note that we haven't talked a lot about pseudo-inverse. It's essentially what you multiply x by to get it to go away, like when you're doing algebra and trying to remove something from an equation.

The formula is $(x'x)^-1 * x'$ (in chunk above).

But, you can also just use `ginv()`.

4. Let's walk through solving GLM with matrix algebra.

First we'll build our GLM: $Y = X\beta + \beta$ 
And use R to solve it.

```{r}
y = c(1:10)

# x is a design matrix. The doc has several, we'll walk through one.
x = matrix(c(rep(1,5), rep(0,10), rep(1,5)), ncol=2, byrow=0)
```

Now we can solve for $\beta$ by doing $\beta = inv(x)*Y$

```{r}
# Inverse of X (actually the 'generalized' or pseudoinverse)
invx = ginv(x)

# Solve for beta using: beta = X^(-1) * y
beta = ginv(x) %*% y
beta

# The model part of the GLM equation (Y = Xb + e), Xb
model = x %*% beta
model

# The error part of the GLM equation (Y = Xb + e), e
e = y - model
e
```

So now we have all the variables: Y, X, b, and e. We can do actual stats!

We'll find the difference from our observations and the grand mean to find our errors.

```{r}
# Sum-of-squares error = e'e, or "t(e)" times e
ss_e = t(e) %*% e
ss_e

# Sum-of-squares model = model'model, after subtracting the mean of the DV
ss_m = t(model-mean(y)) %*% (model-mean(y))
ss_m

# Degrees-of-freedom error is the number of rows - number of columns (r-c) of the design matrix
df_total = size(x,1) - 1
df_e = size(x,1) - size(x,2)  # the "size" function of a matrix wants the matrix (X) and row (1) or column (2) = # rows - # columns
df_e

# Degrees-of-freedom model is the number of columns in X (equivalent to # of rows in the betas) minus 1 = # columns - 1
df_m = size(x,2) - 1
df_m

# Mean-squared error = sum-of-squares error divided by degrees-of-freedom error
# Conceptually: the total amount of error variance divided by the unique number of pieces of information that went into calculating that sum
ms_e = ss_e / df_e
ms_e

# Mean-squared model = sum-of-squares model divided by degrees-of-freedom model
# Conceptually: the total amount of model variance divided by the unique number of pieces of information that went into calculating that sum
ms_m = ss_m / df_m
ms_m

# F is a signal-to-noise ratio where signal = mean-squared model and noise = mean-squared error
F = ms_m / ms_e
F

# p-value is the probability of obtainining an F-value of "F" or greater from a F-distribution
# arguments are f, df_m, df_e, and whether to take the P(F>f) [lower.tail=FALSE] or P(F<f) [lower.tail=TRUE]
pval = pf(F, df_m, df_e, lower.tail = FALSE)
pval
```

This is what's happening under the hood when you run the `lm()` function. It calculates the standard errors, degrees of freedom, F distribution, etc.

How do we know that ratio `ms_m / ms_e` is actually the F distribution? It's assuming that it's the square of normal distribution, but we don't know if it actually is normally distributed.

So what do we do? We can do a bootstrap.

## *Simple bootstrapping using matrices*

Above, we derived our sum-of-squares error in the following steps:

1. Starting with the GLM, $Y = X\beta + e$.
2. Solving for $\beta$ by assuming $e = 0$ then "dividing" both sides by X: $X^{-1}Y = \beta$.
3. Solving for $e$ using: $Y - X\beta = e$.
4. Calculating the sum-of-squares by multiplying $e$ times its transpose: SS-e = $e'e$.

Using this procedure, we decided that our observed sum-of-squares error was 20. 

*NOW*, we want to know if that is a lot or a little. In other words, how often would it actually be less than 20? To decide, we compared the ratio of what our model explained to what was left over ("residual") to a known distribution, $F$. But suppose this parameter didn't follow a known distribution or that we didn't know what it was. Could we still know how big or small the SS-e was? How?

The answer is _bootstrapping_. In this method, we compare our observed parameter (SS-e of 20) to the distribution of observations that would have happened by chance under the null hypothesis, $H_0$. 

What is the null hypothesis? It is that the observations from the two groups are drawn from _the same underlying distribution_, so their true means are expected to be equal. (That's why we want to compare 20 to figure out if it's too small or too big.)

Here's the key insight: under the null, _the distinction between the groups is meaningless_. If all the observations are drawn from the same underlying distribution, then it doesn't matter which observation gets assigned to Group 1 and which to Group 2. This is the premise of bootstrapping.

So how does this work in practice? Bootstrapping has essentially two steps. First is to calculate the parameter (in this case, SS-e) as you normally would. Then, the second step is to _assume the null is true_, shuffle the group assignment, and calculate the paramter a bunch of times to build a distribution. Here's an example:

```{r}
observed_sse = 20
iter = 100000
X = matrix(c(rep(1,5), rep(0,10), rep(1,5)), nrow = 10, byrow=FALSE)

# initiate the SS-e storage vector
ss_e_null = NULL

for (i in 1:iter) {
  
  y_shuffle = sample(y, replace=TRUE)
  b_shuffle = ginv(X) %*% y_shuffle
  e_shuffle = y_shuffle - X %*% b_shuffle
  
  ss_e_null[i] = t(e_shuffle) %*% e_shuffle
}
```

We probably want to plot it now to see what it looks like.

Now that we have our distribution of sums-of-squares error ("sum-of-squares errors?"), we can figure out where our observed value of 20 falls. We can do this visually with a histogram, and also quantitatively with the `ecdf` function. "empirical cumulative density function"()

```{r}
# Put the data into a dataframe
df = data.frame(ss_e_null)

# Use ggplot to plot the histogram
p<-ggplot(df, aes(x=ss_e_null)) + 
  geom_histogram(color="black", fill="aquamarine1", binwidth=2, bins=30) +
  xlab("Observed sum-of-squares error") +
  ylab("Count of observations /(out of 1000/)") +
  geom_vline(aes(xintercept=observed_sse), color="blue",  linetype="dashed", size=1) #what we got "20"
p

# use the "empirical sampling distribution" function to figure out the distribution of the ss_e vector
ss_e_cdf = ecdf(ss_e_null)
p_ss_e_obs = ss_e_cdf(observed_sse)

print(paste("The probability of drawing an SS_e equal to or lower than the observed is",p_ss_e_obs))

plot(ss_e_cdf) #shows you the probability of that "20" based on your data

p2 = ggplot(NULL, aes(x=ss_e_null)) +
  geom_step(stat="ecdf") +
  labs(x= "Sum-of-squared error",y = "Cumulative probability") + 
  geom_vline(aes(xintercept=observed_sse),linetype = "dashed")
p2
```

Now there's no need to ever make assumptions about normality!

Is there utility to doing this? Short answer, yes.
Long answer, it gives you a way to recycle your data and gather new information from it. For example, it's one way of dealing with drawing a sample in your study that is really skewed when you create the sampling distribution from them.

What if we wanted to bootstrap the F distribution?

You do all the same steps you do in ANOVA, but using this shuffling within bootstrap.

```{r}
observed_sse = 20
observed_ssm = 62.5
observed_F = 25
iter = 100000
X = matrix(c(rep(1,5), rep(0,10), rep(1,5)), nrow = 10, byrow=FALSE)

# initiate the F storage vector
F_null = NULL

for (i in 1:iter) {
  
  y_shuffle = sample(y, replace=TRUE)
  b_shuffle = ginv(X) %*% y_shuffle
  e_shuffle = y_shuffle - X %*% b_shuffle
  m_shuffle = X %*% b_shuffle
  
  ss_e_null = t(e_shuffle) %*% e_shuffle
  ss_m_null = t(m_shuffle-mean(y)) %*% (m_shuffle-mean(y))
  
  ms_e_null = ss_e_null / df_e
  ms_m_null = ss_m_null / df_m
  
  F_null[i] = ms_e_null / ms_m_null
}

df = data.frame(F_null)

# Use ggplot to plot the histogram
p<-ggplot(df, aes(x=F_null)) + 
  geom_histogram(color="black", fill="aquamarine1", binwidth=2, bins=30) +
  xlab("Observed F") +
  ylab("Count of observations /(out of 1000/)") +
  geom_vline(aes(xintercept=observed_F), color="blue",  linetype="dashed", size=1)
p
```

We're asking, how often would we get our observed F "25", and our resulting plot tells us, not very often.

## Going over Problem Set 2

In PS2 we'll be simulating the Central Limit Theorem.

The instructions are kind of pseudo-code. They tell you the steps you want to take, letting you know what to code, but doesn't offer code for you to work from.

The BONUS question is to create a nested for loop, that runs the for loop described in steps 3-5 for 20 different sample sizes.

For use in R with bonus question: https://www.rdocumentation.org/packages/SciViews/versions/0.9-13.1/topics/ln